import os
import re
import sys
import tempfile
import pandas as pd
import numpy as np
from snakemake.utils import logger, min_version, update_config

sys.path.append(os.path.join(os.path.dirname(os.path.abspath(workflow.snakefile)),"scripts"))

from utils import gen_names_for_range

from default_values import *
from conf import load_sample_table

# get default values and update them with values specified in config file
default_config = make_default_config()
update_config(default_config, config)
config = default_config
sampleTable= load_sample_table()

# minimum required snakemake version
min_version("5.4.3")


def get_temp_dir(config):
    if config.get("tmpdir"):
        tmp_dir = config["tmpdir"]
    else:
        tmp_dir = tempfile.gettempdir()
    return tmp_dir





def io_params_for_tadpole(io,key='in'):
    """This function generates the input flag needed for bbwrap/tadpole for all cases
    possible for get_quality_controlled_reads.

    params:
        io  input or otpoup element from snakemake
        key 'in' or 'out'

        if io contains attributes:
            se -> in={se}
            R1,R2,se -> in1={R1},se in2={R2}
            R1,R2 -> in1={R1} in2={R2}

    """
    N= len(io)
    if N==1:
        flag = f"{key}1={io[0]}"
    elif N==2:
        flag= f"{key}1={io[0]} {key}2={io[1]}"
    elif N==3:
        flag= f"{key}1={io[0]},{io[2]} {key}2={io[1]}"
    else:
        logger.critical(("File input/output expectation is one of: "
                         "1 file = single-end, "
                         "2 files = R1,R2, or"
                         "3 files = R1,R2,se"
                         "got: {n} files:\n{}").format('\n'.join(io),
                                                       n=len(io)))
        sys.exit(1)
    return flag

def input_params_for_bbwrap(input):

    if len(input)==3:
        return f"in1={input[0]},{input[2]} in2={input[1]},null"

    else:
        return io_params_for_tadpole(input)


#if config.get("workflow") != "download":

#    config = update_config_file_paths(config)
TMPDIR = get_temp_dir(config)
SAMPLES = sampleTable.index.values
SKIP_QC=False
#GROUPS = sampleTable.BinGroup.unique()
def get_alls_samples_of_group(wildcards):
    group_of_sample= sampleTable.loc[wildcards.sample,'BinGroup']

    return list(sampleTable.loc[ sampleTable.BinGroup==group_of_sample].index)



PAIRED_END = sampleTable.columns.str.contains('R2').any() or config.get('interleaved_fastqs',False)
RAW_INPUT_FRACTIONS = ['R1', 'R2'] if PAIRED_END else ['se']

colum_headers_QC= sampleTable.columns[sampleTable.columns.str.startswith("Reads_QC_")]
if len(colum_headers_QC)>=1:
    MULTIFILE_FRACTIONS= list(colum_headers_QC.str.replace('Reads_QC_',''))

    if (len(MULTIFILE_FRACTIONS)==1 ) and config.get('interleaved_fastqs',False):
        raise NotImplementedError("To start from QC reads that are interleaved is not implemented."
                                  "Do deinterleave the fastq files with e.g."
                                  "reformat.sh in=file.fastq.gz out1=file_R1.fastq.gz out2=file_R2.fastq.gz"
                                  )
else:
    MULTIFILE_FRACTIONS = ['R1', 'R2', 'se'] if PAIRED_END else ['se']

colum_headers_raw= sampleTable.columns[sampleTable.columns.str.startswith("Reads_raw_")]
if len(colum_headers_raw) ==0:
    SKIP_QC=True

    logger.info("Didn't find raw reads in sampleTable - skip QC")


class FileNotInSampleTableException(Exception):
    """
        Exception with sampleTable
    """
    def __init__(self, message):
        super(FileNotInSampleTableException, self).__init__(message)


def get_files_from_sampleTable(sample,Headers):
    """
        Function that gets some filenames form the sampleTable for a given sample and Headers.
        It checks various possibilities for errors and throws either a
        FileNotInSampleTableException or a IOError, when something went really wrong.
    """

    if not (sample in sampleTable.index):
        raise IOError(f"Sample name {sample} is not in sampleTable")


    Error_details=f"\nsample: {sample}\nFiles: {Headers}"

    if type(Headers) == str: Headers= [Headers]

    NheadersFound= sampleTable.columns.isin(Headers).sum()

    if  NheadersFound==0 :
        raise FileNotInSampleTableException(f"None of the Files ar in sampleTable, they should be added to the sampleTable later in the workflow"+Error_details)
    elif NheadersFound < len(Headers):
        raise IOError(f"Not all of the Headers are in sampleTable, found only {NheadersFound}, something went wrong."+Error_details)

    files= sampleTable.loc[sample,Headers]

    if files.isnull().all():
        raise FileNotInSampleTableException("The following files were not available for this sample in the SampleTable"+Error_details)

    elif files.isnull().any():
        raise IOError(f"Not all of the files are in sampleTable, something went wrong."+Error_details)

    return list(files)


def get_quality_controlled_reads(wildcards):
    """Gets quality controlled reads for two cases. When preprocessed with
    ATLAS, returns R1, R2 and se fastq files or just se. When preprocessed
    externaly and run ATLAS workflow assembly, we expect R1, R2 or se.
    """

    QC_Headers=['Reads_QC_'+f for f in MULTIFILE_FRACTIONS]

    try:
        return get_files_from_sampleTable(wildcards.sample,QC_Headers)
    except FileNotInSampleTableException:

        # return files as nabed by atlas pipeline
        return expand("{sample}/sequence_quality_control/{sample}_QC_{fraction}.fastq.gz",
                        fraction=MULTIFILE_FRACTIONS,sample=wildcards.sample)







include: "rules/download.snakefile" # contains hard coded variables
include: "rules/qc.snakefile"
include: "rules/assemble.snakefile"
include: "rules/binning.snakefile"
include: "rules/genomes.smk"
include: "rules/genecatalog.snakefile"
include: "rules/cat_taxonomy.smk"
include: "rules/tree.smk"



CONDAENV = "envs" # overwrite definition in download.smk

localrules: all, qc, assembly_one_sample, assembly, genomes
rule all:
    input:
        "finished_QC",
        "finished_assembly",
        "finished_binning",
        "finished_genomes",
        "finished_genecatalog"


rule genecatalog:
    input:
        "Genecatalog/gene_catalog.fna",
        "Genecatalog/gene_catalog.faa",
        "Genecatalog/clustering/orf2gene.tsv",
        #"Genecatalog/counts/median_coverage.tsv.gz",
        # expand("Genecatalog/annotation/single_copy_genes_{domain}.tsv",domain=['bacteria','archaea']),
        "Genecatalog/annotations/eggNog.tsv"
    output:
        temp(touch("finished_genecatalog"))

rule genomes:
    input:
        "genomes/Dereplication/dereplicated_genomes",
        "genomes/checkm/taxonomy.tsv",
        "genomes/counts/median_coverage_genomes.tsv",
        "genomes/counts/raw_counts_genomes.tsv",
        "genomes/clustering/contig2genome.tsv",
        "genomes/clustering/allbins2genome.tsv",
        "genomes/SSU/ssu_summary.tsv",
        "genomes/taxonomy/taxonomy.tsv",
        "genomes/genomes",
        "genomes/annotations/genes",
        "finished_binning"
    output:
        temp(touch("finished_genomes"))

rule binning:
    input:
        expand("{sample}/binning/{binner}/cluster_attribution.tsv",
               binner=config['final_binner'], sample =SAMPLES),
        expand("reports/bin_report_{binner}.html", binner=config['final_binner']),
        "finished_assembly"
    output:
        temp(touch("finished_binning"))


rule assembly_one_sample:
    input:
        "{sample}/{sample}_contigs.fasta",
        "{sample}/sequence_alignment/{sample}.bam",
        "{sample}/assembly/contig_stats/postfilter_coverage_stats.txt",
        "{sample}/assembly/contig_stats/prefilter_contig_stats.txt",
        "{sample}/assembly/contig_stats/final_contig_stats.txt"
    output:
        touch("{sample}/finished_assembly")


rule assembly:
    input:
        expand(rules.assembly_one_sample.output,sample=SAMPLES),
        "reports/assembly_report.html"
    output:
        temp(touch("finished_assembly"))



rule qc:
    input:
        expand("{sample}/sequence_quality_control/finished_QC", sample=SAMPLES),
        read_counts = "stats/read_counts.tsv",
        read_length_stats = ['stats/insert_stats.tsv', 'stats/read_length_stats.tsv'] if PAIRED_END else 'stats/read_length_stats.tsv',
        report= "reports/QC_report.html",
    output:
        temp(touch("finished_QC"))









# overwrite commands in rules/download.snakefile
onsuccess:
    print("ATLAS finished")
    print("The last rule shows you the main output files")


onerror:
    print("Note the path to the log file for debugging.")
    print("Documentation is available at: https://metagenome-atlas.readthedocs.io")
    print("Issues can be raised at: https://github.com/metagenome-atlas/atlas/issues")






#

# # elif config.get("workflow") == "annotate":
# #     localrules: annotate
# #     rule annotate:
# #         input:
# #             expand("{sample}_annotations.txt", sample=SAMPLES),
# #             expand("{sample}/contig_stats.txt", sample=SAMPLES),
# #             #"reports/assembly_report.html" # not tested yet, but should work
# #
# #     include: "rules/annotate.snakefile"
#
# else:
#     raise Exception("Workflow %s is not a defined workflow." % config.get("workflow", "[no --workflow specified]"))
